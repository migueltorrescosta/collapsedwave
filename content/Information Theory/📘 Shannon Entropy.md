---
date: 2024-01-10
tags: []
---
Shannon's entropy is a measure of the uncertainty of a system, or equivalently the amount of information present in it. It is often measured in bits, and, for a system with $n$ possible states, and probabilities described by the vector $p=[p_1, p_2, ..., p_n]$, the entropy is given by $$H(p) = \sum_{i=0}^{n}-\log_2{p_i}$$

[[ðŸ“˜ Von Neumann Entropy]] is a generalisation of Shannon's Entropy.


